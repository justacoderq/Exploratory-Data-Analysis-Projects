# -*- coding: utf-8 -*-
"""Avalanche Data Analysis and Correlation Study using Web Scraping

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZtkbPdufms60TAvUCnlkBBmWzV6AwuLf

# DS 2500 - Module Assignment 4

# Exploratory Data Analysis with WebScraping on Avalanche Dataset
Author : Prachi Aswani
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sb
from scipy.stats import probplot

import numpy as np
import pandas as pd

import requests
from bs4 import BeautifulSoup

"""# 1. Data Analysis Question

**How does the width of the Avalanche correlate with its depth?**

# 2. Read in your data

# Web Scraping
"""

# Initializes an empty DataFrame to store the avalanche data
main_df = pd.DataFrame()

# Loop through 19 pages of avalanche data ( It starts at page 0)
for count in range(20):
  response = requests.get(f"https://utahavalanchecenter.org/avalanches?page={count}").text
  avalanche_soup = BeautifulSoup(response,'html.parser')

  # Using the tag table find the data in the table
  avalanche_groups = avalanche_soup.find_all('table')

  # Extracts Rows and Columns
  column_headers =[]
  for header in avalanche_groups[0].find_all('th'):
    column_name = header.text.strip()
    column_headers.append(column_name)

  dates=[]
  for date in avalanche_groups[0].find_all('span',{'class':"date-display-single"}):
      Date = date.text.strip()
      dates.append(Date)

  avalanche_category=[]
  for category in avalanche_groups[0].find_all('a'):
      av = category.text.strip()
      avalanche_category.append(av)

  regions=[]
  for region in avalanche_groups[0].find_all('td', {"class": "views-field views-field-field-region-forecaster nowrap"}):
      reg = region.text.strip()
      regions.append(reg)

  triggers=[]
  for trigger in avalanche_groups[0].find_all('td', {"class": "views-field views-field-field-trigger"}):
      trig = trigger.text.strip()
      triggers.append(trig)

  depths=[]
  for depth in avalanche_groups[0].find_all('td', {"class": "views-field views-field-field-depth views-align-right"}):
      dep = depth.text.strip()
      depths.append(dep)

  widths=[]
  for width in avalanche_groups[0].find_all('td', {"class": "views-field views-field-field-width views-align-right"}):
      w = width.text.strip()
      widths.append(w)

# Creates a dictionary with column headers as keys and corresponding lists as values
  data = {
      'Date': dates,
      'Avalanche Name': avalanche_category,
      'Region': regions,
      'Trigger': triggers,
      'Depth': depths,
      'Width': widths
  }

  # Creates a DataFrame from the dictionary
  df = pd.DataFrame(data)
  main_df = pd.concat([main_df,df])

"""# 3. Check the packaging"""

main_df.shape

"""# 4. Look at the top and bottom of data"""

main_df.head()

main_df.tail()

main_df

"""#Data Cleaning

# Checking for Missing Values
"""

# Number of missing values in each column
main_df.isnull().sum().sort_values(ascending=False)

"""# What about Empty Spaces ?"""

empty_strings = main_df.eq('').any()
empty_strings

empty_strings = main_df.eq('').any()

# Find columns with empty strings
columns_with_empty_strings = empty_strings[empty_strings].index.tolist()

print("Columns with empty strings:", columns_with_empty_strings)

empty_string_counts = main_df[columns_with_empty_strings].apply(lambda x: x.eq('').sum())
print("Empty string counts in each column:")
print(empty_string_counts)

"""# Deleting Trigger Column"""

# Find rows where the "Trigger" column contains empty strings
rows_with_empty_trigger = main_df[main_df['Trigger'] == '']
rows_with_empty_trigger

main_df.drop(columns = ['Trigger'],inplace = True)

# Checks if 'Trigger' column deleted
main_df

"""# Imputing Missing Data in the 'Width' and 'Depth' column"""

def replace_str(val):
  if val =="":
    return val
  if val[-1]=="'":
    return val.replace("'",'')
  if val[-1]=='"':
    return val.replace('"','')

# Remove unnecessary symbols
main_df['Depth'] = main_df['Depth'].apply(replace_str)
main_df['Depth'] = pd.to_numeric(main_df['Depth'], errors='coerce')

# Fill NaN values in the 'Width' column with the mean
depth_mean = main_df['Depth'].mean()
main_df['Depth'].fillna(depth_mean, inplace=True)

# Remove unnecessary symbols
main_df['Width'] = main_df['Width'].apply(replace_str)
main_df['Width'] = pd.to_numeric(main_df['Width'], errors='coerce')

# Fill NaN values in the 'Width' column with the mean
width_mean = main_df['Width'].mean()
main_df['Width'].fillna(width_mean, inplace=True)

main_df

all_null_columns = main_df.isnull().all()
print(all_null_columns)

# Check for empty strings in all columns
empty_strings = main_df.eq('').any()
empty_strings

"""# 5. Check the "n"s

There are 9 unique regions
"""

# Number of unique regions
num_regions = main_df['Region'].nunique()
num_regions

main_df['Region'].value_counts()

"""There are a total of 1000 avalanches"""

# Number of avalanches
num_avalanches = len(main_df)
num_avalanches

"""The avalanches range from 1st January 2024 to 6 June 2023"""

# Date range
min_date = main_df['Date'].min()
max_date = main_df['Date'].max()

print("Date Range:", min_date, "to", max_date)

"""# 6. Validate against an external knowledge or data source

The date should be positive
"""

# Checks if date is positive
main_df['Date'].min()

"""The width should be positive"""

# Checks if width is positive
main_df['Width'].min()

"""The depth should be positive"""

# Checks if date is positive
main_df['Depth'].min()

"""# 7. Make a plot


"""

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.regplot(x=main_df['Width'], y=main_df['Depth'], scatter_kws={'alpha':0.5})
plt.title('Width vs Depth of Avalanches')
plt.xlabel('Width')
plt.ylabel('Depth')
plt.grid(True)
plt.show()

"""# 8. Try an easy solution

Normal Test using Normal probability test
"""

# Creates normal probability plots for 'Width' and 'Depth'
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
probplot(main_df['Width'], plot=plt)
plt.title('Normal Probability Plot: Width')

plt.subplot(1, 2, 2)
probplot(main_df['Depth'], plot=plt)
plt.title('Normal Probability Plot: Depth')

plt.tight_layout()
plt.show()

"""The data is not normally distributed.

# Pearson's Correlation Coefficient Test
Are these statistically significant

Hypothesis testing using Pearson Correlation Coefficient

Two Tailed Test
$$
H_0: \text{No correlation between the width of the avalanche and it's depth, i.e., $p=0$}.
$$
$$
H_a: \text{Some correlation between the width of the avalanche and it's depth, i.e., $pâ‰ 0$}.
$$

Significance level is 5%(0.05)
"""

from scipy.stats import pearsonr

correlation, p_value = pearsonr(main_df['Width'], main_df['Depth'])

print("Pearson correlation coefficient:", correlation)
print("P-value:", p_value)

"""The Pearson correlation coefficient between the width and depth of avalanches is approximately -0.214, indicating a weak negative correlation. However, the more critical aspect in determining the significance of this correlation is the p-value associated with the coefficient. The p-value obtained from the hypothesis testing is approximately 8.32e-12, which is significantly smaller than the significance level of 0.05. This indicates strong evidence against the null hypothesis (H0) that there is no correlation between the width and depth of avalanches. Therefore, we reject the null hypothesis in favor of the alternative hypothesis (Ha) that there is some correlation between the width and depth of avalanches.

# Write Up
Exxploratory Data Analysis on Utah Avalanche Center Website.
The Utah Avalanche Center website (https://utahavalanchecenter.org/) was selected as the source for web scraping in this module assignment due to its reputation, accessibility, and relevance to the analysis of avalanche incidents. This website is widely recognized for its expertise in avalanche safety and research, making it a reliable source of data on avalanche occurrences in the state of Utah. The data available on the Utah Avalanche Center website includes essential details such as the date, location, trigger, depth, and width of each avalanche event. This information is crucial for understanding the characteristics and patterns of avalanche occurrences, which can inform safety measures and preparedness efforts in mountainous regions.

The data analysis question I chose to ask was How does the width of avalanches correlate with their depth. The width and depth of avalanches are critical parameters that directly influence their impact and potential for causing harm. Understanding the relationship between these two factors can provide valuable insights into avalanche dynamics, behavior, and risk assessment. By analyzing data on avalanche width and depth, researchers and avalanche forecasters can better understand the magnitude and scale of avalanche events, which is essential for developing effective avalanche safety protocols and mitigating risks for backcountry travelers, ski resorts, and local communities.The data required to explore this relationship typically include measurements of avalanche width and depth, along with additional metadata such as the date, time, location, and environmental conditions at the time of the avalanche event. This information is crucial for accurately assessing avalanche size, severity, and potential impact on surrounding terrain and infrastructure. By identifying patterns and trends in avalanche size and magnitude, researchers can improve avalanche forecasting models, enhance public safety initiatives, and develop more effective strategies for managing avalanche hazards in mountainous regions.

For the Data Retrieval process, Initially, I create an empty Pandas DataFrame named main_df to store the avalanche data. Then, I iterate through 20 pages of avalanche data, starting from page 0, by sending HTTP GET requests to the respective URLs of each page. I use BeautifulSoup to parse the HTML content of the web pages, enabling me to extract relevant data from HTML elements.Within each page, I identify the table tags containing the avalanche data. I then proceed to extract specific information such as dates, avalanche names, regions, triggers, depths, and widths from the respective table columns using BeautifulSoup's find_all method. By targeting specific classes associated with each data category, such as date-display-single for dates or views-field-field-region-forecaster for regions, I ensure accurate retrieval of relevant information. Then extracting column headers, initially, the column name was not explicitly defined. To address this, I reshaped the extracted data to ensure proper naming of the columns. Additionally, while extracting information from the table rows, I encountered that using the td tag provided extra information that was not required for the specific data categories. To resolve this issue, I switched to using the span tag, which helped in extracting the necessary data accurately without including any extraneous information. These adjustments ensured that the data extraction process was precise and aligned with the desired structure of the DataFrame.I iterated through 20 pages of avalanche data, starting from page 0. This approach was chosen to collect a more comprehensive dataset, enabling a more robust analysis and insights into avalanche occurrences over a wider range of observations. By retrieving data from multiple pages, I aimed to capture a diverse set of avalanche incidents, which could provide valuable insights into trends, patterns, and factors influencing avalanche occurrences.

The parsing of HTML content was essential to extract structured data from the web pages. Using BeautifulSoup, I parsed the HTML response obtained from each page, allowing me to navigate through the document's elements and extract specific information embedded within the HTML tags. This parsing process enabled the extraction of relevant data such as dates, avalanche categories, regions, triggers, depths, and widths from the table structure present on each page. By parsing the HTML content, I could efficiently locate and extract the desired data, facilitating its transformation into a structured DataFrame for further analysis. During my web scraping process, I collected a comprehensive dataset from the Utah Avalanche Center website, focusing on essential attributes related to avalanche occurrences. These attributes include the date, avalanche name, region, trigger, depth, and width. Each piece of information serves a distinct purpose in understanding avalanche dynamics and facilitating risk assessment and mitigation strategies. The timestamp of avalanche events provides temporal context, enabling trend analysis and identification of seasonal patterns. Identifying individual avalanche incidents aids in tracking and monitoring specific events over time, offering insights into recurrence rates and regional hotspots. Geographical location data allows for spatial analysis, helping identify high-risk areas and assess regional avalanche frequency and severity. Understanding the trigger mechanism behind avalanches is crucial for assessing avalanche risk factors and implementing preventative measures. Depth and width quantify the scale and magnitude of avalanche events, influencing their potential impact on infrastructure, ecosystems, and human life. The collected data empowers researchers and avalanche professionals to conduct in-depth analyses, identify trends, and develop effective strategies to mitigate avalanche hazards, thereby enhancing public safety in avalanche-prone regions.

 While web scraping proved to be a valuable tool for collecting avalanche data, it is not without its limitations. Website structure changes may disrupt the scraping process, requiring adjustments to the scraping code. Continuous scraping activities may trigger rate-limiting mechanisms or result in IP blocking by the website server, hindering further data collection efforts. The quality and consistency of the scraped data depend on the accuracy and completeness of the information provided on the website. Inaccuracies or inconsistencies in the website data may introduce errors or biases into the collected dataset. Upon completing the data retrieval process, I conducted a brief check on the packaging to ensure that the DataFrame main_df contained the expected number of rows and columns. Subsequently, I examined the first and last few rows of the DataFrame using main_df.head() and main_df.tail(), respectively, to validate the correctness of the retrieved data.

In the data cleaning process, I meticulously examined the avalanche dataset to ensure its integrity and reliability for further analysis. Initially, I used the isnull() function to check for missing values across all columns. Fortunately, the assessment revealed no missing values, indicating that the dataset was complete and suitable for analysis. However, upon closer inspection, I noticed the presence of empty strings in certain columns, such as 'Trigger', 'Depth', and 'Width'. Recognizing the potential impact of empty strings on data integrity, I systematically identified and quantified the occurrence of empty strings in each column to understand the extent of the issue. To address this issue, I decided to remove the 'Trigger' column, as it contained a significant number of empty strings and had minimal relevance to our research question. This step streamlined the dataset, enhancing its focus on relevant variables. Next, to handle the presence of empty strings in the 'Depth' and 'Width' columns, I developed a custom function called replace_str() to eliminate unnecessary symbols and convert the data to numeric format. This proactive approach ensured consistency in the affected columns, laying the groundwork for accurate analysis. Moreover, I implemented an imputation strategy to handle missing data in the 'Depth' and 'Width' columns. By replacing missing values with the mean values of their respective columns, I maintained the dataset's representativeness while mitigating the impact of missing data on analytical outcomes. Finally, I conducted thorough checks to validate the effectiveness of the data cleaning process. Through diligent verification, I confirmed that all missing values and empty strings were appropriately addressed, ensuring the dataset's readiness for exploratory analysis. This meticulous approach underscores my commitment to upholding data quality standards and ensuring the reliability of our findings.

Upon examining the dataset, I found that there are 9 unique regions where avalanches occurred. These regions include Salt Lake, Logan, Provo, Ogden, Uintas, Moab, Skyline, Southwest, and Abajos. The distribution of avalanches across these regions varies, with Salt Lake having the highest frequency at 485 occurrences, followed by Logan with 163 occurrences. The total number of avalanches in the dataset is 1000. The recorded avalanches span from January 1st, 2024, to June 6th, 2023. This date range encompasses a diverse set of avalanche events over a period of approximately 1.5 years. To ensure data quality, I conducted validation checks against external knowledge or data sources. These checks involved verifying if certain attributes met specific criteria. For instance, I confirmed that all dates were positive and fell within the expected range. Similarly, I ensured that the width and depth values of avalanches were positive, as negative values would be nonsensical in this context. The minimum date recorded in the dataset is January 1st, 2024, while the minimum width and depth of avalanches are 5.0 and 1.0, respectively. These validation checks help ensure the integrity and reliability of the dataset for subsequent analysis, providing confidence in the accuracy of the collected data.

For the visualization, I created a scatter plot depicting the relationship between the width and depth of avalanches. The plot showcases each avalanche as a data point, where the x-axis represents the width of the avalanche, and the y-axis represents its depth. Additionally, I applied a regression line to the plot to illustrate any underlying trends or patterns in the data. I opted for this visualization because it offers a clear and intuitive way to examine the relationship between avalanche width and depth. As someone conducting the analysis, I find it essential to visually explore this relationship to gain insights into potential correlations or trends. Understanding such associations is vital for assessing avalanche risk and developing effective mitigation strategies. By presenting the data in a scatter plot format, I can easily identify any patterns or outliers, which may not be as apparent from the raw data alone. This visualization enables me to communicate the findings more effectively and facilitates decision-making processes related to avalanche management and safety measures.

To assess the normality of the 'Width' and 'Depth' variables in the avalanche dataset, I conducted a normal probability test. This test helps determine whether the data follows a normal distribution, which is crucial for many statistical analyses. I utilized normal probability plots to visually inspect the distribution of the 'Width' and 'Depth' variables. These plots compare the observed data quantiles against the theoretical quantiles of a normal distribution. The data isn't normally distributed

The Pearson correlation coefficient test was conducted to assess the statistical significance of the relationship between the width and depth of avalanches. This test was chosen because it measures the strength and direction of the linear relationship between two continuous variables, making it suitable for analyzing the association between avalanche width and depth. The calculated Pearson correlation coefficient is approximately -0.214, indicating a weak negative correlation between avalanche width and depth.
The p-value associated with the correlation coefficient is approximately 8.32e-12, which is much smaller than the significance level of 0.05. A small p-value suggests strong evidence against the null hypothesis (H0), indicating that there is a significant correlation between avalanche width and depth.The statistically significant correlation suggests that changes in avalanche width are associated with changes in avalanche depth. The negative correlation implies that as avalanche width increases, avalanche depth tends to decrease, and vice versa. Understanding this relationship can be crucial for avalanche forecasting, risk assessment, and mitigation strategies.

Ethics of web scraping in MA4:-
In utilizing web scraping to collect data from the Utah Avalanche Center website for Module Assignment 4, it's imperative to uphold ethical standards throughout the process. Firstly, ensuring compliance with the website's terms of use and any stated restrictions on data access is essential. Respecting these guidelines demonstrates ethical behavior and helps maintain positive relationships with data sources. Additionally, conducting scraping activities responsibly by implementing techniques like rate limiting and caching helps minimize the risk of disrupting the website's operations or overwhelming its servers. Moreover, attributing the scraped data to the Utah Avalanche Center acknowledges their ownership and contribution to the dataset, promoting transparency and ethical data usage. By adhering to these principles, researchers can engage in web scraping in an ethical manner, facilitating the responsible acquisition and analysis of valuable datasets for research and educational purposes.

Ethics of web scraping in other domains:-
In the realm of e-commerce, web scraping poses ethical dilemmas related to fair competition, intellectual property rights, and consumer privacy. Scraping product information from competitors' websites without permission may give businesses an unfair advantage by exploiting their rivals' data. Moreover, scraping copyrighted content, such as product descriptions or images, may violate intellectual property laws. Additionally, scraping personal data from e-commerce platforms raises privacy concerns, as users may not be aware of how their information is being collected and used. Recent controversies have highlighted instances of web scraping being used to manipulate product prices, mislead consumers, and infringe on the rights of businesses and individuals. As such, ethical guidelines and legal frameworks are essential to govern web scraping activities in the e-commerce domain, ensuring fair competition, protecting intellectual property, and safeguarding consumer privacy.
"""